{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarforever/wtf-langchain/blob/main/03_Data_Connections/03_Data_Connections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MjW9VjNto59d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.1.0  openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LS_efmfC5Hp6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/WTFAcademy/WTF-Langchain/main/01_Hello_Langchain/README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-W2t1v65-Gt"
      },
      "source": [
        "## 加载文档"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e1_VoFqS5GJ4"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"./README.md\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "omltifXH6jc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='---\\ntitle: 03. 数据连接\\ntags:\\n  - openai\\n  - llm\\n  - langchain\\n---\\n\\n# WTF Langchain极简入门: 03. 数据连接\\n\\n最近在学习Langchain框架，顺手写一个“WTF Langchain极简入门”，供小白们使用（编程大佬可以另找教程）。本教程默认以下前提：\\n- 使用Python版本的[Langchain](https://github.com/hwchase17/langchain)\\n- LLM使用OpenAI的模型\\n- Langchain目前还处于快速发展阶段，版本迭代频繁，为避免示例代码失效，本教程统一使用版本 **0.1.0 **\\n\\n根据Langchain的[代码约定](https://github.com/hwchase17/langchain/blob/v0.1.0 /pyproject.toml#L14C1-L14C24)，Python版本 \">=3.8.1,<4.0\"。\\n\\n推特：[@verysmallwoods](https://twitter.com/verysmallwoods)\\n\\n所有代码和教程开源在github: [github.com/sugarforever/wtf-langchain](https://github.com/sugarforever/wtf-langchain)\\n\\n-----\\n\\n## 什么是数据连接？\\n\\nLLM应用往往需要用户特定的数据，而这些数据并不属于模型的训练集。`LangChain` 的数据连接概念，通过提供以下组件，实现用户数据的加载、转换、存储和查询：\\n\\n- 文档加载器：从不同的数据源加载文档\\n- 文档转换器：拆分文档，将文档转换为问答格式，去除冗余文档，等等\\n- 文本嵌入模型：将非结构化文本转换为浮点数数组表现形式，也称为向量\\n- 向量存储：存储和搜索嵌入数据（向量）\\n- 检索器：提供数据查询的通用接口\\n\\n我们通过下一段落的实践，来介绍这些组件的使用。\\n## 数据连接实践\\n\\n在LLM应用连接用户数据时，通常我们会以如下步骤完成：\\n1. 加载文档\\n2. 拆分文档\\n3. 向量化文档分块\\n4. 向量数据存储\\n\\n这样，我们就可以通过向量数据的检索器，来查询用户数据。接下来我们看看每一步的代码实现示例。最后，我们将通过一个完整的示例来演示如何使用数据连接。\\n\\n### 加载文档\\n\\n`Langchain` 提供了多种文档加载器，用于从不同的数据源加载不同类型的文档。比如，我们可以从本地文件系统加载文档，也可以通过网络加载远程数据。想了解 `Langchain` 所支持的所有文档加载器，请参考[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)。\\n\\n在本节课程中，我们将使用最基本的 `TextLoader` 来加载本地文件系统中的文档。代码如下：\\n\\n```python\\nfrom langchain.document_loaders import TextLoader\\n\\nloader = TextLoader(\"./README.md\")\\ndocs = loader.load()\\n```\\n\\n在上述代码中，我们使用 `TextLoader` 加载了本地文件系统中的 `./README.md` 文件。`TextLoader` 的 `load` 方法返回一个 `Document` 对象数组（`Document` 是 `Langchain` 提供的文档类，包含原始内容和元数据）。我们可以通过 `Document` 对象的 `page_content` 属性来访问文档的原始内容。\\n\\n完整代码请参考[本节课程的示例代码](./03_Data_Connections.ipynb)。\\n\\n### 拆分文档\\n\\n拆分文档是文档转换中最常见的操作。拆分文档的目的是将文档拆分为更小的文档块，以便更好地利用模型。当我们要基于长篇文本构建QA应用时，必须将文本分割成块，这样才能在数据查询中，基于相似性找到与问题最接近的文本块。这也是常见的AI问答机器人的工作原理。\\n\\n`Langchain` 提供了多种文档拆分器，用于将文档拆分为更小的文档块。我们逐个看看这些拆分器的使用方法。\\n\\n#### 按字符拆分\\n\\n`CharacterTextSplitter` 是最简单的文档拆分器，它将文档拆分为固定长度的文本块。代码如下：\\n\\n```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter(        \\n    separator = \"\\\\n\\\\n\",\\n    chunk_size = 1000,\\n    chunk_overlap  = 200,\\n    length_function = len,\\n)\\n\\nsplit_docs = text_splitter.split_documents(docs)\\n```\\n\\n#### 拆分代码\\n\\n`RecursiveCharacterTextSplitter` 的 `from_language` 函数，可以根据编程语言的特性，将代码拆分为合适的文本块。代码如下：\\n\\n```pyhon\\nPYTHON_CODE = \"\"\"\\ndef hello_langchain():\\n    print(\"Hello, Langchain!\")\\n\\n# Call the function\\nhello_langchain()\\n\"\"\"\\npython_splitter = RecursiveCharacterTextSplitter.from_language(\\n    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\\n)\\npython_docs = python_splitter.create_documents([PYTHON_CODE])\\npython_docs\\n```\\n\\n#### Markdown文档拆分\\n\\n`MarkdownHeaderTextSplitter` 可以将Markdown文档按照段落结构，基于Markdown语法进行文档分块。代码如下：\\n\\n```python\\nfrom langchain.text_splitter import MarkdownHeaderTextSplitter\\n\\nmarkdown_document = \"# Chapter 1\\\\n\\\\n    ## Section 1\\\\n\\\\nHi this is the 1st section\\\\n\\\\nWelcome\\\\n\\\\n ### Module 1 \\\\n\\\\n Hi this is the first module \\\\n\\\\n ## Section 2\\\\n\\\\n Hi this is the 2nd section\"\\n\\nheaders_to_split_on = [\\n    (\"#\", \"Header 1\"),\\n    (\"##\", \"Header 2\"),\\n    (\"###\", \"Header 3\"),\\n]\\n\\nsplitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\\nsplits = splitter.split_text(markdown_document)\\n```\\n\\n#### 按字符递归拆分\\n\\n这也是对于普通文本的推荐拆分方式。它通过一组字符作为参数，按顺序尝试使用这些字符进行拆分，直到块的大小足够小为止。默认的字符列表是[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]。它尽可能地保持段落，句子，单词的完整，因此尽可能地保证语义完整。\\n\\n```python\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    # Set a really small chunk size, just to show.\\n    chunk_size = 100,\\n    chunk_overlap  = 20,\\n    length_function = len,\\n)\\ntexts = text_splitter.split_documents(docs)\\n```\\n#### 按token拆分\\n\\n语言模型，例如OpenAI，有token限制。在API调用中，不应超过token限制。看来，在将文本分块时，根据token数来拆分也是不错的主意。有许多令牌化工具，因此当计算文本的token数时，应该使用与语言模型相同的令牌化工具。\\n\\n注，OpenAI所使用的是[tiktoken](https://github.com/openai/tiktoken)。\\n\\n```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=100, chunk_overlap=0\\n)\\nsplit_docs = text_splitter.split_documents(docs)\\n```\\n### 向量化文档分块\\n\\n`Langchain` 的 `Embeddings` 类实现与文本嵌入模型进行交互的标准接口。当前市场上有许多嵌入模型提供者（如OpenAI、Cohere、Hugging Face等）。\\n\\n嵌入模型创建文本片段的向量表示。这意味着我们可以在向量空间中处理文本，并进行语义搜索等操作，从而查找在向量空间中最相似的文本片段。\\n\\n注，文本之间的相似度由其向量表示的欧几里得距离来衡量。欧几里得距离是最常见的距离度量方式，也称为L2范数。对于两个n维向量a和b，欧几里得距离可以通过以下公式计算：\\n\\n```math\\nd(a, b) = √((a₁ - b₁)² + (a₂ - b₂)² + ... + (aₙ - bₙ)²)\\n```\\n\\n当使用OpenAI的文本嵌入模型时，我们使用如下代码来创建文本片段的向量表示：\\n\\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings_model = OpenAIEmbeddings(openai_api_key=\"你的有效openai api key\")\\nembeddings = embeddings_model.embed_documents(\\n    [\\n        \"你好!\",\\n        \"Langchain!\",\\n        \"你真棒！\"\\n    ]\\n)\\nembeddings\\n```\\n\\n你应该能看到如下输出：\\n\\n```shell\\n[[0.001767348474591444,\\n  -0.016549955833298362,\\n  0.009669921232251705,\\n  -0.024465152668289573,\\n  -0.04928377577655549,\\n  ...],\\n  [...\\n  -0.026084684286027195,\\n  -0.0023797790465254626,\\n  0.006104789779720747,\\n  ...]]\\n```\\n\\n注，当我们在环境变量中设置了 `OPENAI_API_KEY`，在上述代码 `OpenAIEmbeddings(openai_api_key=\"你的有效openai api key\")` 中，参数 `openai_api_key` 可以省略。\\n\\n```shell\\nexport OPENAI_API_KEY=\"...\"\\n```\\n\\n### 向量数据存储\\n\\n向量数据存储，或成为向量数据库，负责存储文本嵌入的向量表现，并提供向量检索的能力。`Langchain` 提供了多种开源或商业向量数据存储，包括：Chroma, FAISS, Pinecone等。\\n\\n本讲以Chroma为例。\\n\\n#### 存储\\n\\n`Langchain` 提供了 `Chroma` 包装类，封装了chromadb的操作。\\n\\n在进行以下代码执行前，需要安装 `Chroma` 的包：\\n\\n```shell\\npip install -q chromadb\\n```\\n\\n```python\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\n\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ndocuments = text_splitter.split_documents(docs)\\ndb = Chroma.from_documents(documents, OpenAIEmbeddings())\\n```\\n\\n#### 检索\\n\\n向量数据库提供的重要接口就是相似性查询。如上述内容提到，文本相似性的衡量，由文本的向量表示的欧几里得距离来衡量。向量数据库提供了该接口，用于查询与给定文本最相似的文本。\\n\\n```python\\nquery = \"什么是WTF Langchain？\"\\ndocs = db.similarity_search(query)\\nprint(docs[0].page_content)\\n```\\n\\n## 总结\\n本节课程中，我们简要介绍了 `Langchain` 的数据连接概念，并完成了以下任务：\\n1. 了解常见的文档加载器，\\n2. 了解常见的文档拆分方法\\n3. 掌握如何利用OpenAI实现文本的向量化和向量数据的存储与查询。\\n\\n### 相关文档资料链接：\\n1. [Python Langchain官方文档](https://python.langchain.com/docs/get_started/introduction.html) \\n2. [本节课程的示例代码](./03_Data_Connections.ipynb)\\n', metadata={'source': './README.md'})]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRPF6Mfn6Ake"
      },
      "source": [
        "## 拆分文档"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FakX37SB6DT4"
      },
      "source": [
        "### 按字符拆分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0gm-A-_r5Wfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6200\n",
            "934\n",
            "999\n",
            "969\n",
            "729\n",
            "958\n",
            "986\n",
            "908\n",
            "467\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "print(len(docs[0].page_content))\n",
        "for split_doc in split_docs:\n",
        "  print(len(split_doc.page_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8avQDR6u6HCP"
      },
      "source": [
        "### 拆分代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OlNC7pR15Z0r"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='def hello_langchain():', metadata={}),\n",
              " Document(page_content='print(\"Hello, Langchain!\")', metadata={}),\n",
              " Document(page_content='# Call the function\\nhello_langchain()', metadata={})]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
        "\n",
        "PYTHON_CODE = \"\"\"\n",
        "def hello_langchain():\n",
        "    print(\"Hello, Langchain!\")\n",
        "\n",
        "# Call the function\n",
        "hello_langchain()\n",
        "\"\"\"\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
        ")\n",
        "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
        "python_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_wWWlWS6JkO"
      },
      "source": [
        "### Markdown文档拆分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Gg6twioR5cX8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Chapter 1\n",
            "\n",
            "    ## Section 1\n",
            "\n",
            "Hi this is the 1st section\n",
            "\n",
            "Welcome\n",
            "\n",
            " ### Module 1 \n",
            "\n",
            " Hi this is the first module \n",
            "\n",
            " ## Section 2\n",
            "\n",
            " Hi this is the 2nd section\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Hi this is the 1st section  \\nWelcome', metadata={'Header 1': 'Chapter 1', 'Header 2': 'Section 1'}),\n",
              " Document(page_content='Hi this is the first module', metadata={'Header 1': 'Chapter 1', 'Header 2': 'Section 1', 'Header 3': 'Module 1'}),\n",
              " Document(page_content='Hi this is the 2nd section', metadata={'Header 1': 'Chapter 1', 'Header 2': 'Section 2'})]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "markdown_document = \"# Chapter 1\\n\\n    ## Section 1\\n\\nHi this is the 1st section\\n\\nWelcome\\n\\n ### Module 1 \\n\\n Hi this is the first module \\n\\n ## Section 2\\n\\n Hi this is the 2nd section\"\n",
        "print(markdown_document)\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]\n",
        "\n",
        "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "splits = splitter.split_text(markdown_document)\n",
        "\n",
        "splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spo_Nn036Oko"
      },
      "source": [
        "### 按字符递归拆分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RLxIWV3G5nSh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6200\n",
            "---\n",
            "title: 03. 数据连接\n",
            "tags:\n",
            "  - openai\n",
            "  - llm\n",
            "  - langchain\n",
            "---\n",
            "\n",
            "# WTF Langchain极简入门: 03. 数据连接\n",
            "93\n",
            "71\n",
            "81\n",
            "78\n",
            "99\n",
            "30\n",
            "15\n",
            "56\n",
            "17\n",
            "86\n",
            "18\n",
            "94\n",
            "88\n",
            "38\n",
            "33\n",
            "64\n",
            "82\n",
            "90\n",
            "36\n",
            "75\n",
            "49\n",
            "59\n",
            "59\n",
            "93\n",
            "88\n",
            "43\n",
            "58\n",
            "99\n",
            "45\n",
            "67\n",
            "68\n",
            "67\n",
            "93\n",
            "50\n",
            "63\n",
            "95\n",
            "80\n",
            "41\n",
            "67\n",
            "58\n",
            "79\n",
            "92\n",
            "72\n",
            "97\n",
            "95\n",
            "97\n",
            "78\n",
            "51\n",
            "12\n",
            "84\n",
            "47\n",
            "76\n",
            "98\n",
            "71\n",
            "63\n",
            "99\n",
            "37\n",
            "60\n",
            "67\n",
            "98\n",
            "66\n",
            "93\n",
            "65\n",
            "82\n",
            "67\n",
            "39\n",
            "59\n",
            "72\n",
            "89\n",
            "29\n",
            "11\n",
            "81\n",
            "86\n",
            "59\n",
            "98\n",
            "38\n",
            "52\n",
            "98\n",
            "97\n",
            "36\n",
            "59\n",
            "56\n",
            "99\n",
            "71\n",
            "47\n",
            "61\n",
            "93\n",
            "72\n",
            "31\n",
            "79\n",
            "50\n",
            "13\n",
            "90\n",
            "43\n",
            "最近在学习Langchain框架，顺手写一个“WTF Langchain极简入门”，供小白们使用（编程大佬可以另找教程）。本教程默认以下前提：\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_documents(docs)\n",
        "print(len(docs[0].page_content))\n",
        "print(texts[0].page_content)\n",
        "print(texts[1].page_content)\n",
        "for split_doc in texts:\n",
        "  \n",
        "  print(len(split_doc.page_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH_AHoif6SVQ"
      },
      "source": [
        "### 按token拆分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "L0zcXo2y8urg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WGg-ZOaq5pzl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 258, which is longer than the specified 100\n",
            "Created a chunk of size 158, which is longer than the specified 100\n",
            "Created a chunk of size 290, which is longer than the specified 100\n",
            "Created a chunk of size 119, which is longer than the specified 100\n",
            "Created a chunk of size 154, which is longer than the specified 100\n",
            "Created a chunk of size 223, which is longer than the specified 100\n",
            "Created a chunk of size 209, which is longer than the specified 100\n",
            "Created a chunk of size 259, which is longer than the specified 100\n",
            "Created a chunk of size 216, which is longer than the specified 100\n",
            "Created a chunk of size 112, which is longer than the specified 100\n",
            "Created a chunk of size 192, which is longer than the specified 100\n",
            "Created a chunk of size 112, which is longer than the specified 100\n",
            "Created a chunk of size 140, which is longer than the specified 100\n",
            "Created a chunk of size 174, which is longer than the specified 100\n",
            "Created a chunk of size 133, which is longer than the specified 100\n",
            "Created a chunk of size 127, which is longer than the specified 100\n",
            "Created a chunk of size 108, which is longer than the specified 100\n",
            "Created a chunk of size 157, which is longer than the specified 100\n",
            "Created a chunk of size 177, which is longer than the specified 100\n",
            "Created a chunk of size 202, which is longer than the specified 100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='---\\ntitle: 03. 数据连接\\ntags:\\n  - openai\\n  - llm\\n  - langchain\\n---\\n\\n# WTF Langchain极简入门: 03. 数据连接', metadata={'source': './README.md'}),\n",
              " Document(page_content='最近在学习Langchain框架，顺手写一个“WTF Langchain极简入门”，供小白们使用（编程大佬可以另找教程）。本教程默认以下前提：\\n- 使用Python版本的[Langchain](https://github.com/hwchase17/langchain)\\n- LLM使用OpenAI的模型\\n- Langchain目前还处于快速发展阶段，版本迭代频繁，为避免示例代码失效，本教程统一使用版本 **0.1.0 **', metadata={'source': './README.md'}),\n",
              " Document(page_content='根据Langchain的[代码约定](https://github.com/hwchase17/langchain/blob/v0.1.0 /pyproject.toml#L14C1-L14C24)，Python版本 \">=3.8.1,<4.0\"。', metadata={'source': './README.md'}),\n",
              " Document(page_content='推特：[@verysmallwoods](https://twitter.com/verysmallwoods)\\n\\n所有代码和教程开源在github: [github.com/sugarforever/wtf-langchain](https://github.com/sugarforever/wtf-langchain)\\n\\n-----', metadata={'source': './README.md'}),\n",
              " Document(page_content='## 什么是数据连接？', metadata={'source': './README.md'}),\n",
              " Document(page_content='LLM应用往往需要用户特定的数据，而这些数据并不属于模型的训练集。`LangChain` 的数据连接概念，通过提供以下组件，实现用户数据的加载、转换、存储和查询：', metadata={'source': './README.md'}),\n",
              " Document(page_content='- 文档加载器：从不同的数据源加载文档\\n- 文档转换器：拆分文档，将文档转换为问答格式，去除冗余文档，等等\\n- 文本嵌入模型：将非结构化文本转换为浮点数数组表现形式，也称为向量\\n- 向量存储：存储和搜索嵌入数据（向量）\\n- 检索器：提供数据查询的通用接口', metadata={'source': './README.md'}),\n",
              " Document(page_content='我们通过下一段落的实践，来介绍这些组件的使用。\\n## 数据连接实践', metadata={'source': './README.md'}),\n",
              " Document(page_content='在LLM应用连接用户数据时，通常我们会以如下步骤完成：\\n1. 加载文档\\n2. 拆分文档\\n3. 向量化文档分块\\n4. 向量数据存储', metadata={'source': './README.md'}),\n",
              " Document(page_content='这样，我们就可以通过向量数据的检索器，来查询用户数据。接下来我们看看每一步的代码实现示例。最后，我们将通过一个完整的示例来演示如何使用数据连接。', metadata={'source': './README.md'}),\n",
              " Document(page_content='### 加载文档', metadata={'source': './README.md'}),\n",
              " Document(page_content='`Langchain` 提供了多种文档加载器，用于从不同的数据源加载不同类型的文档。比如，我们可以从本地文件系统加载文档，也可以通过网络加载远程数据。想了解 `Langchain` 所支持的所有文档加载器，请参考[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)。', metadata={'source': './README.md'}),\n",
              " Document(page_content='在本节课程中，我们将使用最基本的 `TextLoader` 来加载本地文件系统中的文档。代码如下：\\n\\n```python\\nfrom langchain.document_loaders import TextLoader', metadata={'source': './README.md'}),\n",
              " Document(page_content='loader = TextLoader(\"./README.md\")\\ndocs = loader.load()\\n```', metadata={'source': './README.md'}),\n",
              " Document(page_content='在上述代码中，我们使用 `TextLoader` 加载了本地文件系统中的 `./README.md` 文件。`TextLoader` 的 `load` 方法返回一个 `Document` 对象数组（`Document` 是 `Langchain` 提供的文档类，包含原始内容和元数据）。我们可以通过 `Document` 对象的 `page_content` 属性来访问文档的原始内容。', metadata={'source': './README.md'}),\n",
              " Document(page_content='完整代码请参考[本节课程的示例代码](./03_Data_Connections.ipynb)。\\n\\n### 拆分文档', metadata={'source': './README.md'}),\n",
              " Document(page_content='拆分文档是文档转换中最常见的操作。拆分文档的目的是将文档拆分为更小的文档块，以便更好地利用模型。当我们要基于长篇文本构建QA应用时，必须将文本分割成块，这样才能在数据查询中，基于相似性找到与问题最接近的文本块。这也是常见的AI问答机器人的工作原理。', metadata={'source': './README.md'}),\n",
              " Document(page_content='`Langchain` 提供了多种文档拆分器，用于将文档拆分为更小的文档块。我们逐个看看这些拆分器的使用方法。', metadata={'source': './README.md'}),\n",
              " Document(page_content='#### 按字符拆分\\n\\n`CharacterTextSplitter` 是最简单的文档拆分器，它将文档拆分为固定长度的文本块。代码如下：', metadata={'source': './README.md'}),\n",
              " Document(page_content='```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter(        \\n    separator = \"\\\\n\\\\n\",\\n    chunk_size = 1000,\\n    chunk_overlap  = 200,\\n    length_function = len,\\n)', metadata={'source': './README.md'}),\n",
              " Document(page_content='split_docs = text_splitter.split_documents(docs)\\n```\\n\\n#### 拆分代码', metadata={'source': './README.md'}),\n",
              " Document(page_content='`RecursiveCharacterTextSplitter` 的 `from_language` 函数，可以根据编程语言的特性，将代码拆分为合适的文本块。代码如下：', metadata={'source': './README.md'}),\n",
              " Document(page_content='```pyhon\\nPYTHON_CODE = \"\"\"\\ndef hello_langchain():\\n    print(\"Hello, Langchain!\")', metadata={'source': './README.md'}),\n",
              " Document(page_content='# Call the function\\nhello_langchain()\\n\"\"\"\\npython_splitter = RecursiveCharacterTextSplitter.from_language(\\n    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\\n)\\npython_docs = python_splitter.create_documents([PYTHON_CODE])\\npython_docs\\n```\\n\\n#### Markdown文档拆分', metadata={'source': './README.md'}),\n",
              " Document(page_content='`MarkdownHeaderTextSplitter` 可以将Markdown文档按照段落结构，基于Markdown语法进行文档分块。代码如下：\\n\\n```python\\nfrom langchain.text_splitter import MarkdownHeaderTextSplitter', metadata={'source': './README.md'}),\n",
              " Document(page_content='markdown_document = \"# Chapter 1\\\\n\\\\n    ## Section 1\\\\n\\\\nHi this is the 1st section\\\\n\\\\nWelcome\\\\n\\\\n ### Module 1 \\\\n\\\\n Hi this is the first module \\\\n\\\\n ## Section 2\\\\n\\\\n Hi this is the 2nd section\"', metadata={'source': './README.md'}),\n",
              " Document(page_content='headers_to_split_on = [\\n    (\"#\", \"Header 1\"),\\n    (\"##\", \"Header 2\"),\\n    (\"###\", \"Header 3\"),\\n]\\n\\nsplitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\\nsplits = splitter.split_text(markdown_document)\\n```', metadata={'source': './README.md'}),\n",
              " Document(page_content='#### 按字符递归拆分', metadata={'source': './README.md'}),\n",
              " Document(page_content='这也是对于普通文本的推荐拆分方式。它通过一组字符作为参数，按顺序尝试使用这些字符进行拆分，直到块的大小足够小为止。默认的字符列表是[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]。它尽可能地保持段落，句子，单词的完整，因此尽可能地保证语义完整。', metadata={'source': './README.md'}),\n",
              " Document(page_content='```python\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    # Set a really small chunk size, just to show.\\n    chunk_size = 100,\\n    chunk_overlap  = 20,\\n    length_function = len,\\n)\\ntexts = text_splitter.split_documents(docs)\\n```\\n#### 按token拆分', metadata={'source': './README.md'}),\n",
              " Document(page_content='语言模型，例如OpenAI，有token限制。在API调用中，不应超过token限制。看来，在将文本分块时，根据token数来拆分也是不错的主意。有许多令牌化工具，因此当计算文本的token数时，应该使用与语言模型相同的令牌化工具。', metadata={'source': './README.md'}),\n",
              " Document(page_content='注，OpenAI所使用的是[tiktoken](https://github.com/openai/tiktoken)。', metadata={'source': './README.md'}),\n",
              " Document(page_content='```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=100, chunk_overlap=0\\n)\\nsplit_docs = text_splitter.split_documents(docs)\\n```\\n### 向量化文档分块', metadata={'source': './README.md'}),\n",
              " Document(page_content='`Langchain` 的 `Embeddings` 类实现与文本嵌入模型进行交互的标准接口。当前市场上有许多嵌入模型提供者（如OpenAI、Cohere、Hugging Face等）。', metadata={'source': './README.md'}),\n",
              " Document(page_content='嵌入模型创建文本片段的向量表示。这意味着我们可以在向量空间中处理文本，并进行语义搜索等操作，从而查找在向量空间中最相似的文本片段。', metadata={'source': './README.md'}),\n",
              " Document(page_content='注，文本之间的相似度由其向量表示的欧几里得距离来衡量。欧几里得距离是最常见的距离度量方式，也称为L2范数。对于两个n维向量a和b，欧几里得距离可以通过以下公式计算：', metadata={'source': './README.md'}),\n",
              " Document(page_content='```math\\nd(a, b) = √((a₁ - b₁)² + (a₂ - b₂)² + ... + (aₙ - bₙ)²)\\n```', metadata={'source': './README.md'}),\n",
              " Document(page_content='当使用OpenAI的文本嵌入模型时，我们使用如下代码来创建文本片段的向量表示：', metadata={'source': './README.md'}),\n",
              " Document(page_content='```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings_model = OpenAIEmbeddings(openai_api_key=\"你的有效openai api key\")\\nembeddings = embeddings_model.embed_documents(\\n    [\\n        \"你好!\",\\n        \"Langchain!\",\\n        \"你真棒！\"\\n    ]\\n)\\nembeddings\\n```', metadata={'source': './README.md'}),\n",
              " Document(page_content='你应该能看到如下输出：', metadata={'source': './README.md'}),\n",
              " Document(page_content='```shell\\n[[0.001767348474591444,\\n  -0.016549955833298362,\\n  0.009669921232251705,\\n  -0.024465152668289573,\\n  -0.04928377577655549,\\n  ...],\\n  [...\\n  -0.026084684286027195,\\n  -0.0023797790465254626,\\n  0.006104789779720747,\\n  ...]]\\n```', metadata={'source': './README.md'}),\n",
              " Document(page_content='注，当我们在环境变量中设置了 `OPENAI_API_KEY`，在上述代码 `OpenAIEmbeddings(openai_api_key=\"你的有效openai api key\")` 中，参数 `openai_api_key` 可以省略。', metadata={'source': './README.md'}),\n",
              " Document(page_content='```shell\\nexport OPENAI_API_KEY=\"...\"\\n```\\n\\n### 向量数据存储', metadata={'source': './README.md'}),\n",
              " Document(page_content='向量数据存储，或成为向量数据库，负责存储文本嵌入的向量表现，并提供向量检索的能力。`Langchain` 提供了多种开源或商业向量数据存储，包括：Chroma, FAISS, Pinecone等。', metadata={'source': './README.md'}),\n",
              " Document(page_content='本讲以Chroma为例。\\n\\n#### 存储\\n\\n`Langchain` 提供了 `Chroma` 包装类，封装了chromadb的操作。', metadata={'source': './README.md'}),\n",
              " Document(page_content='在进行以下代码执行前，需要安装 `Chroma` 的包：\\n\\n```shell\\npip install -q chromadb\\n```', metadata={'source': './README.md'}),\n",
              " Document(page_content='```python\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma', metadata={'source': './README.md'}),\n",
              " Document(page_content='text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ndocuments = text_splitter.split_documents(docs)\\ndb = Chroma.from_documents(documents, OpenAIEmbeddings())\\n```\\n\\n#### 检索', metadata={'source': './README.md'}),\n",
              " Document(page_content='向量数据库提供的重要接口就是相似性查询。如上述内容提到，文本相似性的衡量，由文本的向量表示的欧几里得距离来衡量。向量数据库提供了该接口，用于查询与给定文本最相似的文本。', metadata={'source': './README.md'}),\n",
              " Document(page_content='```python\\nquery = \"什么是WTF Langchain？\"\\ndocs = db.similarity_search(query)\\nprint(docs[0].page_content)\\n```', metadata={'source': './README.md'}),\n",
              " Document(page_content='## 总结\\n本节课程中，我们简要介绍了 `Langchain` 的数据连接概念，并完成了以下任务：\\n1. 了解常见的文档加载器，\\n2. 了解常见的文档拆分方法\\n3. 掌握如何利用OpenAI实现文本的向量化和向量数据的存储与查询。', metadata={'source': './README.md'}),\n",
              " Document(page_content='### 相关文档资料链接：\\n1. [Python Langchain官方文档](https://python.langchain.com/docs/get_started/introduction.html) \\n2. [本节课程的示例代码](./03_Data_Connections.ipynb)', metadata={'source': './README.md'})]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100, chunk_overlap=0\n",
        ")\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "split_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8dfw22O6Vb2"
      },
      "source": [
        "## 向量化文档分块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AghMYu8r5zBW"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings_model = OpenAIEmbeddings(\n",
        "    # openai_api_key=\"\"\n",
        "    )\n",
        "embeddings = embeddings_model.embed_documents(\n",
        "    [\n",
        "        \"你好!\",\n",
        "        \"Langchain!\",\n",
        "        \"你真棒！\"\n",
        "    ]\n",
        ")\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY\n"
          ]
        }
      ],
      "source": [
        "!echo OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYq8gm4g6ZBl"
      },
      "source": [
        "## 向量数据存储"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jff1dIkk6cwh"
      },
      "source": [
        "### 存储"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3KT-ziYSMMn9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m Checking for Rust toolchain....\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Cargo, the Rust package manager, is not installed or is not on PATH.\n",
            "  \u001b[31m   \u001b[0m This package requires Rust and Cargo to compile extensions. Install it through\n",
            "  \u001b[31m   \u001b[0m the system's package manager or via https://rustup.rs/\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q chromadb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtDRMAx752w_"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "db = Chroma.from_documents(documents, OpenAIEmbeddings(openai_api_key=\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SmtLL016f5l"
      },
      "source": [
        "### 检索"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqqP4P4554j5"
      },
      "outputs": [],
      "source": [
        "query = \"什么是WTF Langchain？\"\n",
        "docs = db.similarity_search(query)\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAa13Y7DM-rO"
      },
      "outputs": [],
      "source": [
        "docs = db.similarity_search_with_score(query)\n",
        "docs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOgQoOin53yoGqil3iR6M6W",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
